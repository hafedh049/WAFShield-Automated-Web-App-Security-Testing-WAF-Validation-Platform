---
# K3s HA Cluster Ansible Playbook (Final - Production-ready)
# Supports: Debian/Ubuntu & RHEL-like (RHEL 9/10, Rocky, Alma)
# Inventory must define hosts: CP1, CP2, CP3, W1, W2, W3, W4
# Each host may have ansible_host set (optional). Playbook uses gathered default IPv4 if ansible_host missing.

- name: Deploy HA K3s Cluster
  hosts: all
  gather_facts: true
  become: true
  vars:
    ansible_user: root
    k3s_version: "v1.33.5+k3s1"
    cp_nodes:
      - CP1
      - CP2
      - CP3
    worker_nodes:
      - W1
      - W2
      - W3
      - W4
    # local file where we save the kubeconfig fetched from CP1
    kubeconfig_local_path: "./kubeconfig"

  tasks:
    # -----------------------------
    # Install prerequisites
    # -----------------------------
    - name: Ensure basic packages present (Debian)
      apt:
        name:
          - curl
          - wget
          - ca-certificates
          - gnupg
        state: present
        update_cache: true
      when: ansible_os_family == 'Debian'

    - name: Ensure basic packages present (RedHat)
      yum:
        name:
          - curl
          - wget
          - ca-certificates
          - gnupg2
        state: present
      when: ansible_os_family == 'RedHat'

    # -----------------------------
    # SELinux: make permissive permanently (RHEL family)
    # -----------------------------
    - name: Disable SELinux permanently (RHEL family)
      block:
        - name: Set SELinux to permissive now
          command: setenforce 0
          when: ansible_selinux.status == "enabled"
          ignore_errors: true

        - name: Persist SELinux permissive in config
          replace:
            path: /etc/selinux/config
            regexp: "^SELINUX=.*"
            replace: "SELINUX=permissive"
          when: ansible_selinux.status == "enabled"
      when: ansible_os_family == 'RedHat'

    # -----------------------------
    # Firewall configuration (RHEL)
    # -----------------------------
    - name: Ensure firewalld installed and open K3s ports (RHEL)
      block:
        - yum:
            name: firewalld
            state: present
        - service:
            name: firewalld
            state: started
            enabled: true
        - firewalld:
            port: "{{ item }}"
            permanent: true
            state: enabled
          loop:
            - 6443/tcp
            - 2379-2380/tcp
            - 10250/tcp
            - 8472/udp
        - command: firewall-cmd --reload
      when: ansible_os_family == 'RedHat'

    - name: Disable ufw (Debian)
      block:
        - stat:
            path: /usr/sbin/ufw
          register: ufw_stat
        - command: ufw disable
          when: ufw_stat.stat.exists
          ignore_errors: true
      when: ansible_os_family == 'Debian'

    # -----------------------------
    # Hostnames: set unique hostnames and reboot if changed
    # -----------------------------
    - name: Set unique hostname (use inventory name)
      hostname:
        name: "{{ inventory_hostname | lower }}"
      register: hostname_result

    - name: Reboot if hostname changed
      reboot:
        msg: "Rebooting after hostname change"
        pre_reboot_delay: 2
      when: hostname_result.changed

    # -----------------------------
    # Cleanup old K3s data and stop services (only if present)
    # -----------------------------
    - name: Check if k3s service exists
      shell: systemctl list-unit-files k3s.service --no-pager --no-legend
      register: k3s_service_check
      changed_when: false
      failed_when: false

    - name: Stop k3s service if present
      service:
        name: k3s
        state: stopped
      when: k3s_service_check.stdout != ""
      ignore_errors: true

    - name: Check if k3s-agent service exists
      shell: systemctl list-unit-files k3s-agent.service --no-pager --no-legend
      register: k3s_agent_check
      changed_when: false
      failed_when: false

    - name: Stop k3s-agent service if present
      service:
        name: k3s-agent
        state: stopped
      when: k3s_agent_check.stdout != ""
      ignore_errors: true

    - name: Remove old k3s directories (safe)
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - /etc/rancher/k3s
        - /var/lib/rancher/k3s
        - /etc/rancher/node/password
      ignore_errors: true

    # -----------------------------
    # Install k3sup on Ansible control (localhost) - run once
    # -----------------------------
    - name: Install k3sup (localhost)
      delegate_to: localhost
      run_once: true
      shell: |
        set -e
        command -v k3sup >/dev/null 2>&1 || curl -sLS https://get.k3sup.dev | sh

    # -----------------------------
    # Bootstrap CP1 (run from localhost)
    # -----------------------------
    - name: Bootstrap CP1 (k3sup install)
      delegate_to: localhost
      run_once: true
      shell: |
        set -euo pipefail
        IP={{ hostvars['CP1'].get('ansible_host', hostvars['CP1']['ansible_default_ipv4']['address']) }}
        echo "Bootstrapping CP1 $IP"
        k3sup install \
          --ip $IP \
          --user {{ ansible_user }} \
          --cluster \
          --k3s-version {{ k3s_version }}
      environment:
        K3SUP_SSH_KEY: ""

    - name: Wait for K3s API on CP1 (TCP 6443)
      delegate_to: localhost
      wait_for:
        host: "{{ hostvars['CP1'].get('ansible_host', hostvars['CP1']['ansible_default_ipv4']['address']) }}"
        port: 6443
        delay: 5
        timeout: 180

    # -----------------------------
    # Pod readiness: run on CP1 (server-local kubeconfig)
    # -----------------------------
    - name: Wait until all critical K3s pods are running (on CP1)
      delegate_to: CP1
      shell: |
        set -e
        KUBECONFIG="/etc/rancher/k3s/k3s.yaml"
        PENDING_PODS=$(KUBECONFIG="$KUBECONFIG" kubectl get pods -A --no-headers | grep -v 'Running\|Completed' || true)
        if [ -n "$PENDING_PODS" ]; then
          echo "Some pods are not yet ready:"
          echo "$PENDING_PODS"
          exit 1
        fi
      register: pods_status
      retries: 30
      delay: 10
      until: pods_status.rc == 0

    # -----------------------------
    # Fetch kubeconfig from CP1 to localhost and fix server IP
    # -----------------------------
    - name: Fetch kubeconfig from CP1 to localhost
      copy:
        src: /etc/rancher/k3s/k3s.yaml
        dest: "{{ kubeconfig_local_path }}"
        remote_src: true
      run_once: true
      when: inventory_hostname == 'CP1'

    - name: Adjust kubeconfig to use CP1 default IPv4 (local)
      delegate_to: localhost
      run_once: true
      shell: |
        set -e
        IP="{{ hostvars['CP1'].get('ansible_host', hostvars['CP1']['ansible_default_ipv4']['address']) }}"
        sed -i "s/127.0.0.1/$IP/g" "{{ kubeconfig_local_path }}"
        chmod 600 "{{ kubeconfig_local_path }}"

    # -----------------------------
    # Join additional control planes (CP2 & CP3) with retries
    # -----------------------------
    - name: Join CP nodes (CP2 & CP3) with retries
      delegate_to: localhost
      run_once: false
      when: inventory_hostname in cp_nodes and inventory_hostname != 'CP1'
      register: join_cp_result
      retries: 6
      delay: 15
      until: join_cp_result.rc == 0
      shell: |
        set -euo pipefail
        IP={{ hostvars[inventory_hostname].get('ansible_host', hostvars[inventory_hostname]['ansible_default_ipv4']['address']) }}
        SERVER_IP={{ hostvars['CP1'].get('ansible_host', hostvars['CP1']['ansible_default_ipv4']['address']) }}
        echo "Joining {{ inventory_hostname }} as control plane (IP=$IP) to server $SERVER_IP"
        k3sup join \
          --ip $IP \
          --server-ip $SERVER_IP \
          --user {{ ansible_user }} \
          --server \
          --k3s-version {{ k3s_version }}

    # -----------------------------
    # Join worker nodes with retries
    # -----------------------------
    - name: Join worker nodes with retries
      delegate_to: localhost
      run_once: false
      when: inventory_hostname in worker_nodes
      register: worker_join_status
      retries: 10
      delay: 12
      until: worker_join_status.rc == 0
      shell: |
        set -euo pipefail
        IP={{ hostvars[inventory_hostname].get('ansible_host', hostvars[inventory_hostname]['ansible_default_ipv4']['address']) }}
        SERVER_IP={{ hostvars['CP1'].get('ansible_host', hostvars['CP1']['ansible_default_ipv4']['address']) }}
        echo "Joining worker {{ inventory_hostname }} (IP=$IP) to server $SERVER_IP"
        k3sup join \
          --ip $IP \
          --server-ip $SERVER_IP \
          --user {{ ansible_user }} \
          --k3s-version {{ k3s_version }}

    # -----------------------------
    # Label and taint nodes (run from localhost using fetched kubeconfig)
    # -----------------------------
    - name: Ensure kubeconfig_local_path exists (on localhost)
      delegate_to: localhost
      stat:
        path: "{{ kubeconfig_local_path }}"
      register: kubeconfig_stat
      run_once: true

    - name: Fail if kubeconfig is missing
      delegate_to: localhost
      fail:
        msg: "Kubeconfig {{ kubeconfig_local_path }} not found. CP1 may not be ready."
      when: not kubeconfig_stat.stat.exists
      run_once: true

    - name: Label and taint all nodes (control-plane + workers)
      delegate_to: localhost
      run_once: true
      loop: "{{ cp_nodes + worker_nodes }}"
      vars:
        lower_item: "{{ item | lower }}"
        cp_join_list: "{{ cp_nodes | map('lower') | list | join(' ') }}"
      shell: |
        #!/bin/bash
        set -euo pipefail

        NODE_NAME="{{ lower_item }}"
        KUBECONFIG_PATH="{{ kubeconfig_local_path }}"

        export KUBECONFIG="$KUBECONFIG_PATH"

        # Label nodes (control-plane/etcd for CPs, worker for workers)
        kubectl label node $NODE_NAME node-role.kubernetes.io/control-plane={{ 'true' if item in cp_nodes else 'false' }} --overwrite || true
        kubectl label node $NODE_NAME node-role.kubernetes.io/etcd={{ 'true' if item in cp_nodes else 'false' }} --overwrite || true
        kubectl label node $NODE_NAME node-role.kubernetes.io/worker={{ 'true' if item in worker_nodes else 'false' }} --overwrite || true

        # Taint control-plane nodes so they do not schedule regular pods
        if echo "{{ cp_join_list }}" | grep -qw "{{ lower_item }}"; then
          kubectl taint node $NODE_NAME node-role.kubernetes.io/control-plane=:NoSchedule --overwrite || true
        else
          # Remove taint from workers if somehow present
          kubectl taint node $NODE_NAME node-role.kubernetes.io/control-plane- || true
        fi

    # -----------------------------
    # Final: show nodes (from localhost)
    # -----------------------------
    - name: Show cluster nodes
      delegate_to: localhost
      run_once: true
      shell: |
        KUBECONFIG="{{ kubeconfig_local_path }}" kubectl get nodes -o wide

# End of playbook
